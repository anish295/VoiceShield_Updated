#!/usr/bin/env python3
"""
AI-Powered Aggression Detection Server
Handles real-time audio and facial analysis for emotion detection
"""

import asyncio
import json
import logging
import numpy as np
import websockets
from flask import Flask, render_template
from flask_socketio import SocketIO, emit
import threading
import time
from collections import deque
import base64
import os
from flask_cors import CORS
import io
import soundfile as sf
import librosa
import torch
import torchaudio
from transformers import pipeline
from scipy.stats import mode
                'facialScore': round(combined_result['facial_score'], 1),
                'audioScore': round(combined_result['audio_score'], 1),
                'faceCount': aggregated_facial_result.get('face_count', 1),
                'emotionBars': {k: round(v, 1) for k, v in aggregated_facial_result.get('emotion_bars', {}).items()},
                'audioBars': {k: round(v, 1) for k, v in audio_result.get('audio_bars', {}).items()},
                'multiVoice': int(audio_result.get('multi_voice', False)) all SpeechBrain and related model loading code

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optional VAD import
try:
    import webrtcvad
    HAS_VAD = True
except ImportError:
    HAS_VAD = False
    logger.warning("webrtcvad not available - voice activity detection will be disabled")

# Flask app setup
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
CORS(app, origins=["*"])
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# Global variables for storing analysis data
facial_data_buffer = deque(maxlen=10)
audio_data_buffer = deque(maxlen=50)
analysis_results = {}

class EmotionAnalyzer:
    """AI-powered emotion analyzer for combined audio and facial analysis"""
    
    def __init__(self):
        self.facial_weights = {
            'angry': 0.7,
            'fearful': 0.2,
            'disgusted': 0.1
        }
        
        # Voice emotion classifier using a better model for emotion detection
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            # Using audeering emotion recognition model (better for speech emotions)
            self.voice_classifier = pipeline(
                task='audio-classification',
                model='audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim',
                device=device,
                top_k=None
            )
            logger.info(f"Loaded voice emotion model on {device}")
        except Exception as e:
            logger.error(f"Failed to load voice emotion model: {e}")
            self.voice_classifier = None
            
        # Initialize confidence thresholds (lower thresholds for better sensitivity)
        self.confidence_thresholds = {
            'angry': 0.45,    # Reduced from 0.65
            'happy': 0.40,    # Reduced from 0.55
            'sad': 0.40,      # Reduced from 0.55
            'fearful': 0.45,  # Reduced from 0.60
            'disgusted': 0.45,# Reduced from 0.60
            'surprised': 0.40,# Reduced from 0.55
            'neutral': 0.35   # Reduced from 0.45
        }

        # Initialize voice activity detection thresholds
        self.vad_energy_threshold = 0.01
        self.vad_duration_threshold = 0.2

        # Emotion history for temporal smoothing
        self.emotion_history = {
            'angry': deque(maxlen=5),
            'happy': deque(maxlen=5),
            'sad': deque(maxlen=5),
            'fearful': deque(maxlen=5),
            'disgusted': deque(maxlen=5),
            'surprised': deque(maxlen=5),
            'neutral': deque(maxlen=5)
        }
        
        # Dynamic confidence thresholds
        self.confidence_thresholds = {
            'angry': 0.65,      # Higher threshold for angry to reduce false positives
            'happy': 0.55,
            'sad': 0.55,
            'fearful': 0.60,
            'disgusted': 0.60,
            'surprised': 0.55,
            'neutral': 0.45     # Lower threshold for neutral
        }
        
        # Temporal smoothing parameters
        self.smoothing_window = 5  # Number of frames to consider
        self.decay_factor = 0.85   # Exponential decay for older predictions
        
    def analyze_facial_emotions(self, expressions):
        """Analyze facial expressions for aggression indicators"""
        if not expressions:
            return {'aggression_score': 0, 'confidence': 0, 'emotions': {}}
        
        aggression_score = 0
        for emotion, weight in self.facial_weights.items():
            if emotion in expressions:
                aggression_score += expressions[emotion] * weight * 100
        
        if aggression_score < 10 and (
            expressions.get('angry', 0) > 0.005 or
            expressions.get('fearful', 0) > 0.005 or
            expressions.get('disgusted', 0) > 0.005
        ):
            aggression_score = 10
        
        max_expression = max(expressions.values()) if expressions else 0
        confidence = min(max_expression * 100, 95)
        # Convert to percentages and round to one decimal place
        emotion_bars = {k: round(v * 100, 1) for k, v in expressions.items()}
        
        return {
            'aggression_score': round(min(aggression_score, 100), 1),
            'confidence': round(confidence, 1),
            'emotions': {k: round(v * 100, 1) for k, v in expressions.items()},
            'emotion_bars': emotion_bars
        }

    def analyze_audio_emotions(self, audio_data_b64: str):
        """Analyze 16kHz mono WAV base64 string and return voice emotion scores with improved accuracy."""
        if not audio_data_b64 or not self.voice_classifier:
            return {'voice_emotion': 'Unknown', 'aggression_score': 0, 'confidence': 0, 'audio_bars': {}, 'multi_voice': False}
        try:
            # Decode and load audio
            wav_bytes = base64.b64decode(audio_data_b64)
            audio_io = io.BytesIO(wav_bytes)
            samples, sr = sf.read(audio_io, dtype='float32', always_2d=False)
            
            # Validate audio input
            if len(samples) == 0:
                logger.warning("Empty audio input received")
                return self._get_neutral_result()
            
            # Convert to mono if stereo
            if samples.ndim > 1:
                samples = np.mean(samples, axis=1)
            
            # Resample to 16kHz if needed
            if sr != 16000:
                samples = librosa.resample(samples, orig_sr=sr, target_sr=16000)
                sr = 16000
                
            # Audio preprocessing pipeline
            # 1. Normalize audio
            samples = librosa.util.normalize(samples)
            
            # 2. Remove silence and trim
            samples, _ = librosa.effects.trim(samples, top_db=20)
            
            # 3. Apply pre-emphasis filter to enhance high frequencies
            samples = librosa.effects.preemphasis(samples, coef=0.97)
            
            # 4. Check if audio is too quiet or too short
            rms = np.sqrt(np.mean(np.square(samples)))
            duration = len(samples) / sr
            
            if rms < 0.01 or duration < 0.2:  # Too quiet or too short
                logger.info("Audio input too quiet or too short")
                return self._get_neutral_result()
                
            # Normalize audio
            samples = librosa.util.normalize(samples)
            
            # Apply pre-emphasis filter
            samples = librosa.effects.preemphasis(samples)
            
            # Voice Activity Detection using energy-based approach
            rms = np.sqrt(np.mean(np.square(samples)))
            duration = len(samples) / sr
            
            if rms < self.vad_energy_threshold or duration < self.vad_duration_threshold:
                neutral_bars = {e: 0.0 for e in ['angry', 'happy', 'sad', 'fearful', 'disgusted', 'surprised', 'neutral']}
                neutral_bars['neutral'] = 100.0
                return {
                    'voice_emotion': 'neutral',
                    'aggression_score': 0.0,
                    'confidence': 95.0,  # High confidence that there's no speech
                    'audio_bars': neutral_bars,
                    'multi_voice': False,
                    'no_input': True
                }
            # Silence / no-voice detection via RMS and peak
            if len(samples) == 0:
                return {'voice_emotion': 'No input', 'aggression_score': 0.0, 'confidence': 0.0, 'audio_bars': {}, 'multi_voice': False, 'no_input': True}
            rms = float(np.sqrt(np.mean(np.square(samples))))
            peak = float(np.max(np.abs(samples))) if samples.size else 0.0
            duration_sec = len(samples) / float(sr)
            if rms < 0.01 or peak < 0.02 or duration_sec < 0.25:
                neutral_bars = {e: 0.0 for e in ['angry','happy','sad','fearful','disgusted','surprised','neutral']}
                neutral_bars['neutral'] = 100.0
                return {
                    'voice_emotion': 'neutral',
                    'aggression_score': 0.0,
                    'confidence': 0.0,
                    'audio_bars': neutral_bars,
                    'multi_voice': False,
                    'no_input': True
                }
            # Extract audio features
            mfcc = librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=13)
            spectral_contrast = librosa.feature.spectral_contrast(y=samples, sr=sr)
            
            # Extract acoustic features
            features = self._extract_acoustic_features(samples, sr)
            
            # Run classifier with enhanced features
            result = self.voice_classifier({
                'array': samples,
                'sampling_rate': sr,
                'features': features
            }, top_k=None)
            
            # Initialize target emotions
            target_emotions = ['angry', 'happy', 'sad', 'fearful', 'disgusted', 'surprised', 'neutral']
            scores = {e: 0.0 for e in target_emotions}
            
            # Map and normalize emotion scores
            emotion_mapping = {
                'fear': 'fearful',
                'disgust': 'disgusted',
                'surprise': 'surprised',
                'happiness': 'happy',
                'anger': 'angry',
                'sadness': 'sad',
                'calm': 'neutral'
            }
            
            # Process classifier results with improved mapping
            raw_scores = {}
            for item in result:
                label = item['label'].lower()
                score = float(item['score'])
                
                # Enhanced emotion mapping
                if 'ang' in label or 'mad' in label or 'rage' in label:
                    target_label = 'angry'
                elif 'hap' in label or 'joy' in label or 'excit' in label:
                    target_label = 'happy'
                elif 'sad' in label or 'depress' in label or 'unhap' in label:
                    target_label = 'sad'
                elif 'fear' in label or 'nerv' in label or 'worr' in label:
                    target_label = 'fearful'
                elif 'disgust' in label or 'unpleas' in label:
                    target_label = 'disgusted'
                elif 'surpr' in label or 'amaz' in label:
                    target_label = 'surprised'
                elif 'neut' in label or 'calm' in label:
                    target_label = 'neutral'
                else:
                    continue
                
                if target_label in scores:
                    raw_scores[target_label] = raw_scores.get(target_label, 0) + score
            
            # Normalize raw scores
            total_score = sum(raw_scores.values()) if raw_scores else 1
            for emotion in scores:
                scores[emotion] = raw_scores.get(emotion, 0) / total_score
            
            # Apply adaptive thresholding
            max_score = max(scores.values())
            if max_score > 0:
                for emotion in scores:
                    # Only keep emotions that are at least 30% of the max score
                    if scores[emotion] < max_score * 0.3:
                        scores[emotion] = 0
                
                # Renormalize after thresholding
                total = sum(scores.values()) or 1
                for emotion in scores:
                    scores[emotion] = scores[emotion] / total
            
            # Apply temporal smoothing with faster response to changes
            for emotion in scores:
                self.emotion_history[emotion].append(scores[emotion])
                # Use shorter history for more responsive detection
                recent_scores = list(self.emotion_history[emotion])[-3:]
                scores[emotion] = np.mean(recent_scores)
            
            # Normalize scores
            total = sum(scores.values())
            if total > 0:
                for k in scores:
                    scores[k] = scores[k] / total
            else:
                scores['neutral'] = 1.0
            
            # Enhanced emotion classification using acoustic features
            energy = librosa.feature.rms(y=samples)[0].mean()
            zero_crossing_rate = librosa.feature.zero_crossing_rate(y=samples)[0].mean()
            pitch = librosa.yin(samples, fmin=75, fmax=300)[0].mean()
            
            # Adjust emotion scores based on acoustic features
            if energy > 0.2 and zero_crossing_rate > 0.15:  # High energy and variability
                if scores['angry'] > 0.3:
                    scores['angry'] *= 1.2  # Boost anger detection
                if scores['happy'] > 0.3:
                    scores['happy'] *= 1.1  # Boost happiness detection
            elif energy < 0.1 and zero_crossing_rate < 0.1:  # Low energy and variability
                if scores['sad'] > 0.3:
                    scores['sad'] *= 1.2  # Boost sadness detection
                if scores['fearful'] > 0.3:
                    scores['fearful'] *= 1.1  # Boost fear detection
            
            # Normalize scores after adjustment
            total = sum(scores.values())
            if total > 0:
                for emotion in scores:
                    scores[emotion] = scores[emotion] / total
            
            # Multi-voice detection using spectral features
            harmonic, percussive = librosa.effects.hpss(samples)
            multi_voice = False
            if librosa.feature.spectral_flatness(y=harmonic).mean() > 0.5:
                multi_voice = True

            # Convert scores to percentages with one decimal place
            audio_bars = {k: round(v * 100, 1) for k, v in scores.items()}
            
            # Process emotion scores
            sorted_emotions = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            top_emotion = sorted_emotions[0][0]
            top_score = sorted_emotions[0][1]
            second_score = sorted_emotions[1][1] if len(sorted_emotions) > 1 else 0
            
            # Calculate confidence margin
            confidence_margin = top_score - second_score
            
            # Enhanced confidence calculation based on acoustic features
            energy_confidence = min(1.0, features['energy']['rms_mean'] * 10)
            pitch_confidence = min(1.0, abs(features['pitch']['mean'] - 120) / 100)
            spectral_confidence = min(1.0, features['spectral']['contrast'])
            
            # Weighted confidence score
            confidence_weights = {
                'classifier': 0.5,
                'energy': 0.2,
                'pitch': 0.15,
                'spectral': 0.15
            }
            
            base_confidence = (
                top_score * confidence_weights['classifier'] +
                energy_confidence * confidence_weights['energy'] +
                pitch_confidence * confidence_weights['pitch'] +
                spectral_confidence * confidence_weights['spectral']
            ) * 100
            
            # Adjust confidence based on emotion-specific thresholds
            if top_emotion == 'angry':
                # Higher requirements for angry classification
                if (base_confidence < self.confidence_thresholds['angry'] or
                    confidence_margin < 0.15 or
                    features['energy']['rms_mean'] < 0.1):
                    top_emotion = 'neutral'
                    base_confidence = 95.0
            elif top_emotion != 'neutral' and base_confidence < self.confidence_thresholds[top_emotion]:
                top_emotion = 'neutral'
                base_confidence = 90.0
                
            # Final confidence score
            confidence = min(95.0, base_confidence)
                
                # Calculate aggression score with weighted features
            # Calculate acoustic-based aggression score
            energy_score = features['energy']['rms_mean'] * 100
            pitch_deviation = abs(features['pitch']['mean'] - 120) / 2
            spectral_intensity = features['spectral']['contrast'] * 100
            
            # Calculate final aggression score with multiple components
            aggression_score = round(
                (scores['angry'] * 0.4) +                # Primary emotion weight
                (scores['fearful'] * 0.15) +            # Secondary emotion weight
                (scores['disgusted'] * 0.15) +          # Secondary emotion weight
                (energy_score / 100 * 0.15) +           # Energy component
                (pitch_deviation / 100 * 0.1) +         # Pitch deviation component
                (spectral_intensity / 100 * 0.05),      # Spectral intensity component
                1
            )
            
            # Check for multiple voices
            multi_voice = self._is_multi_voice(samples, sr)
            
            # Convert scores to audio bars format
            audio_bars = {k: round(v * 100, 1) for k, v in scores.items()}
            
            # Prepare the final result
            return {
                'voice_emotion': top_emotion,
                'aggression_score': float(min(100, aggression_score)),
                'confidence': float(confidence),
                'audio_bars': audio_bars,
                'multi_voice': multi_voice,
                'no_input': False,
                'acoustic_features': {
                    'energy': float(energy_score),
                    'pitch': float(pitch_deviation),
                    'spectral': float(spectral_intensity),
                    'confidence_margin': float(confidence_margin)
                }
            }
        except Exception as e:
            logger.error(f"Error analyzing audio: {e}", exc_info=True)
            return {'voice_emotion': 'Unknown', 'aggression_score': 0, 'confidence': 0, 'audio_bars': {}, 'multi_voice': False}
    
    def _get_neutral_result(self):
        """Return a neutral emotion result with high confidence."""
        return {
            'voice_emotion': 'neutral',
            'aggression_score': 0.0,
            'confidence': 95.0,
            'audio_bars': {e: (100.0 if e == 'neutral' else 0.0) for e in 
                         ['angry', 'happy', 'sad', 'fearful', 'disgusted', 'surprised', 'neutral']},
            'multi_voice': False,
            'no_input': True
        }

    def _extract_acoustic_features(self, samples, sr):
        """Extract comprehensive acoustic features for emotion detection."""
        features = {}
        
        # 1. Mel-frequency cepstral coefficients (MFCCs)
        mfccs = librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=13)
        features['mfcc'] = mfccs
        
        # 2. Spectral features
        spectral_centroids = librosa.feature.spectral_centroid(y=samples, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=samples, sr=sr)[0]
        spectral_contrast = librosa.feature.spectral_contrast(y=samples, sr=sr)
        features['spectral'] = {
            'centroid': np.mean(spectral_centroids),
            'rolloff': np.mean(spectral_rolloff),
            'contrast': np.mean(spectral_contrast)
        }
        
        # 3. Rhythm features
        tempo, _ = librosa.beat.beat_track(y=samples, sr=sr)
        features['rhythm'] = {'tempo': tempo}
        
        # 4. Energy features
        rms = librosa.feature.rms(y=samples)[0]
        zcr = librosa.feature.zero_crossing_rate(y=samples)[0]
        features['energy'] = {
            'rms_mean': np.mean(rms),
            'rms_std': np.std(rms),
            'zcr_mean': np.mean(zcr),
            'zcr_std': np.std(zcr)
        }
        
        # 5. Pitch features
        pitches, magnitudes = librosa.piptrack(y=samples, sr=sr)
        pitch_mean = np.mean(pitches[magnitudes > np.max(magnitudes)*0.1])
        features['pitch'] = {
            'mean': pitch_mean,
            'std': np.std(pitches[magnitudes > np.max(magnitudes)*0.1])
        }
        
        return features

    def _is_multi_voice(self, samples, sr):
        """Detect if multiple voices are present in the audio."""
        # 1. Spectral flatness analysis
        flatness = librosa.feature.spectral_flatness(y=samples)[0]
        mean_flatness = np.mean(flatness)
        
        # 2. Pitch analysis
        pitches, magnitudes = librosa.piptrack(y=samples, sr=sr)
        significant_pitches = pitches[magnitudes > np.max(magnitudes)*0.1]
        
        # 3. Check pitch variance
        if len(significant_pitches) > 0:
            pitch_std = np.std(significant_pitches)
            if pitch_std > 50:  # High pitch variance suggests multiple speakers
                return True
        
        # 4. Check spectral flatness
        if mean_flatness > 0.5:  # High flatness suggests multiple sound sources
            return True
        
        return False

    def combine_analysis(self, facial_result, audio_result):
        """Combine facial and audio analysis for comprehensive aggression detection"""
        facial_weight = 0.6
        audio_weight = 0.4
        
        facial_score = facial_result.get('aggression_score', 0)
        audio_score = audio_result.get('aggression_score', 0)
        combined_score = (facial_score * facial_weight) + (audio_score * audio_weight)
        
        facial_confidence = facial_result.get('confidence', 0)
        audio_confidence = audio_result.get('confidence', 0)
        combined_confidence = (facial_confidence * facial_weight) + (audio_confidence * audio_weight)
        
        if combined_score > 80:
            recommendation = "High risk - Immediate intervention recommended"
        elif combined_score > 60:
            recommendation = "Moderate risk - Monitor closely"
        elif combined_score > 40:
            recommendation = "Low risk - Continue monitoring"
        else:
            recommendation = "No action needed"
        
        return {
            'combined_score': round(combined_score, 1),
            'combined_confidence': round(combined_confidence, 1),
            'facial_score': facial_score,
            'audio_score': audio_score,
            'voice_emotion': audio_result.get('voice_emotion', 'Unknown'),
            'recommendation': recommendation
        }

# Initialize the emotion analyzer
emotion_analyzer = EmotionAnalyzer()

@socketio.on('connect')
def handle_connect():
    logger.info('Client connected')
    emit('status', {'message': 'Connected to AI server'})

@socketio.on('disconnect')
def handle_disconnect():
    logger.info('Client disconnected')

@socketio.on('facial_analysis')
def handle_facial_analysis(data):
    """Handle incoming facial analysis data"""
    try:
        expressions = data.get('expressions', {})
        # ... (rest of the function is unchanged)
        face_index = data.get('faceIndex', 0)
        
        facial_data_buffer.append({
            'expressions': expressions,
            'timestamp': time.time(),
            'face_index': face_index
        })
        
        current_time = time.time()
        facial_data_buffer_list = list(facial_data_buffer)
        facial_data_buffer.clear()
        facial_data_buffer.extend([entry for entry in facial_data_buffer_list if current_time - entry['timestamp'] < 5.0])
        
        facial_result = emotion_analyzer.analyze_facial_emotions(expressions)
        analysis_results[f'facial_face_{face_index}'] = facial_result
        
        # Get facial results sorted by face index
        facial_results = [(int(k.split('_')[-1]), v) for k, v in analysis_results.items() 
                         if k.startswith('facial_face_') and isinstance(v, dict)]
        facial_results.sort(key=lambda x: x[0])  # Sort by face index
        all_facial_results = [v for _, v in facial_results]  # Get sorted results
        if all_facial_results:
            avg_aggression_score = sum(r.get('aggression_score', 0) for r in all_facial_results) / len(all_facial_results)
            avg_confidence = sum(r.get('confidence', 0) for r in all_facial_results) / len(all_facial_results)
            emotion_keys = set()
            for r in all_facial_results:
                emotion_keys.update(r.get('emotion_bars', {}).keys())
            # Round all percentage values to one decimal place
            avg_emotion_bars = {k: round(sum(r.get('emotion_bars', {}).get(k, 0) for r in all_facial_results) / len(all_facial_results), 1) for k in emotion_keys}
            aggregated_facial_result = {
                'aggression_score': round(avg_aggression_score, 1),
                'confidence': round(avg_confidence, 1),
                'emotions': {k: round(v * 100, 1) for k, v in expressions.items()},
                'emotion_bars': avg_emotion_bars,
                'face_count': len(all_facial_results)
            }
            analysis_results['facial'] = aggregated_facial_result
        else:
            aggregated_facial_result = facial_result
        
        if 'audio' in analysis_results and analysis_results.get('audio'):
            combined_result = emotion_analyzer.combine_analysis(
                aggregated_facial_result,
                analysis_results['audio']
            )
            # Round all numeric values to one decimal place
            emit('emotion_analysis', {
                'type': 'emotion_analysis',
                'voiceEmotion': combined_result['voice_emotion'],
                'combinedScore': round(float(combined_result['combined_score']), 1),
                'confidence': round(float(combined_result['combined_confidence']), 1),
                'recommendation': combined_result['recommendation'],
                'facialScore': round(float(combined_result['facial_score']), 1),
                'audioScore': round(float(combined_result['audio_score']), 1),
                'faceCount': aggregated_facial_result.get('face_count', 1),
                'emotionBars': {k: round(float(v), 1) for k, v in aggregated_facial_result.get('emotion_bars', {}).items()},
                'audioBars': {k: round(float(v), 1) for k, v in analysis_results['audio'].get('audio_bars', {}).items()},
                'multiVoice': int(analysis_results['audio'].get('multi_voice', False))
            })
        else:
            # Round all numeric values to one decimal place for consistency
            emit('emotion_analysis', {
                'type': 'emotion_analysis',
                'voiceEmotion': 'Analyzing...',
                'combinedScore': round(float(aggregated_facial_result.get('aggression_score', 0)), 1),
                'confidence': round(float(aggregated_facial_result.get('confidence', 0)), 1),
                'recommendation': f'Monitoring {aggregated_facial_result.get("face_count", 1)} face(s)...',
                'facialScore': round(float(aggregated_facial_result.get('aggression_score', 0)), 1),
                'audioScore': 0.0,
                'faceCount': aggregated_facial_result.get('face_count', 1),
                'emotionBars': {k: round(float(v), 1) for k, v in aggregated_facial_result.get('emotion_bars', {}).items()}
            })

    except Exception as e:
        logger.error(f"Error processing facial analysis: {e}", exc_info=True)
        emit('error', {'message': 'Error processing facial analysis'})

@socketio.on('audio')
def handle_audio_data(data):
    """Handle incoming audio data"""
    try:
        # If the client indicates no input, emit neutral quickly and return
        if data.get('no_input'):
            analysis_results['audio'] = {
                'voice_emotion': 'neutral',
                'aggression_score': 0.0,
                'confidence': 0.0,
                'audio_bars': {e: (100.0 if e == 'neutral' else 0.0) for e in ['angry','happy','sad','fearful','disgusted','surprised','neutral']},
                'multi_voice': False,
                'no_input': True
            }
            emit('emotion_analysis', {
                'type': 'emotion_analysis',
                'voiceEmotion': 'neutral',
                'combinedScore': analysis_results.get('facial', {}).get('aggression_score', 0),
                'confidence': 0.0,
                'recommendation': 'No voice input',
                'facialScore': analysis_results.get('facial', {}).get('aggression_score', 0),
                'audioScore': 0.0,
                'faceCount': analysis_results.get('facial', {}).get('face_count', 0),
                'emotionBars': analysis_results.get('facial', {}).get('emotion_bars', {}),
                'audioBars': analysis_results['audio'].get('audio_bars', {}),
                'multiVoice': 0
            })
            return

        audio_data = data.get('data')
        # ... (rest of the function is unchanged)
        audio_data_buffer.append({
            'data': audio_data,
            'timestamp': time.time()
        })
        audio_result = emotion_analyzer.analyze_audio_emotions(audio_data)
        analysis_results['audio'] = audio_result

        if 'facial' in analysis_results and analysis_results.get('facial'):
            combined_result = emotion_analyzer.combine_analysis(
                analysis_results['facial'],
                audio_result
            )
            emit('emotion_analysis', {
                'type': 'emotion_analysis',
                'voiceEmotion': combined_result['voice_emotion'],
                'combinedScore': combined_result['combined_score'],
                'confidence': combined_result['combined_confidence'],
                'recommendation': combined_result['recommendation'],
                'facialScore': combined_result['facial_score'],
                'audioScore': combined_result['audio_score'],
                'faceCount': analysis_results['facial'].get('face_count', 1),
                'emotionBars': analysis_results['facial'].get('emotion_bars', {}),
                'audioBars': audio_result.get('audio_bars', {}),
                'multiVoice': int(audio_result.get('multi_voice', False))
            })
        else:
            emit('emotion_analysis', {
                'type': 'emotion_analysis',
                'voiceEmotion': audio_result.get('voice_emotion', 'Unknown'),
                'combinedScore': audio_result.get('aggression_score', 0),
                'confidence': audio_result.get('confidence', 0),
                'recommendation': 'Monitoring voice patterns...',
                'facialScore': 0,
                'audioScore': audio_result.get('aggression_score', 0),
                'faceCount': 0,
                'emotionBars': {},
                'audioBars': audio_result.get('audio_bars', {}),
                'multiVoice': int(audio_result.get('multi_voice', False))
            })
    except Exception as e:
        logger.error(f"Error processing audio data: {e}", exc_info=True)
        emit('error', {'message': 'Error processing audio data'})


@app.route('/')
def index():
    # This route is now optional if you open index.html directly
    return render_template('index.html')

def run_server():
    """Run the Flask server with SocketIO"""
    logger.info("Starting AI Aggression Detection Server...")
    port = int(os.environ.get("PORT", 5000))  # Changed to port 5000
    print(f"Server starting at http://localhost:{port}")
    socketio.run(app, host='0.0.0.0', port=port, debug=True, allow_unsafe_werkzeug=True)

if __name__ == '__main__':
    run_server()